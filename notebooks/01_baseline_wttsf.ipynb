{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6768,"databundleVersionId":44342,"sourceType":"competition"}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport gc \nimport warnings \nfrom dataclasses import dataclass \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nwarnings.filterwarnings(\"ignore\")\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\n\nDATA_DIR = \"/kaggle/input/web-traffic-time-series-forecasting\"\nOUT_PATH = \"/kaggle/working/submission.csv\"\n\nTRAIN_1 = os.path.join(DATA_DIR, \"train_1.csv.zip\")\nTRAIN_2 = os.path.join(DATA_DIR, \"train_2.csv.zip\")\nKEY_1 = os.path.join(DATA_DIR, \"key_1.csv.zip\")\nKEY_2 = os.path.join(DATA_DIR, \"key_2.csv.zip\")\nSAMPLE_1 = os.path.join(DATA_DIR, \"sample_submission_1.csv.zip\")\nSAMPLE_2 = os.path.join(DATA_DIR, \"sample_submission_2.csv.zip\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Defined some functions below to assist me, adhering to competition requirements. ","metadata":{}},{"cell_type":"code","source":"def smape(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-8) -> float:\n    \"\"\"\n    SMAPE = 2 * |y - yhat| / (|y| + |yhat|)\n    \"\"\"\n    y_true = np.asarray(y_true, dtype=np.float64)\n    y_pred = np.asarray(y_pred, dtype=np.float64)\n    denom = np.abs(y_true) + np.abs(y_pred) + eps #to account for when it is zero in log1 space\n    return float(np.mean(2.0 * np.abs(y_pred - y_true) / denom))\n    \ndef detect_date_columns(df: pd.DataFrame, id_col: str = \"Page\") -> list[str]:\n    \"\"\"\n    because training files are in wide format\n    \"\"\"\n    cols = [c for c in df.columns if c != id_col] # Keep only columns that parse as dates\n    parsed = pd.to_datetime(cols, errors=\"coerce\")\n    date_cols = [c for c, d in zip(cols, parsed) if not pd.isna(d)] # Sort by actual datetime\n    date_cols = sorted(date_cols, key=lambda c: pd.to_datetime(c))\n    return date_cols\n\ndef extract_date_from_page(key: pd.DataFrame) -> pd.DataFrame:\n    page_parts = key[\"page\"].str.rsplit(\"_\", n=1, expand=True)\n    key[\"page\"] = page_parts[0]\n    key[\"date\"] = pd.to_datetime(page_parts[1], errors=\"coerce\")\n\n    if key[\"date\"].isna().any(): #shld not happen \n        bad = key[key[\"date\"].isna()].head(3)\n        raise ValueError(\n            \"Some dates could not be parsed from Page column. Example rows:\"\n            + bad.to_string(index=False)\n        )\n    return key \n\ndef read_key(path) -> pd.DataFrame:\n    key = pd.read_csv(path, compression=\"zip\")\n\n    # standardising column names jic\n    cols_lower = {c: c.lower() for c in key.columns}\n    key = key.rename(columns=cols_lower)\n\n    if \"id\" not in key.columns:\n        raise ValueError(f\"Key file at {path} did not contain an 'Id' column. Found: {list(key.columns)}\")\n    if \"page\" not in key.columns:\n        raise ValueError(f\"Key file at {path} did not contain a 'Page' column. Found: {list(key.columns)}\")\n\n    key = extract_date_from_page(key)\n\n    return key[[\"id\", \"page\", \"date\"]]\n\ndef concat_and_dedupe_predictions(preds_list, keep=\"first\", verbose=True):\n    \"\"\"\n    To enforce unique ids in the submission: I saw that key_1 and key_2 can contain overlapping Id.\n    So ill be removing them deterministically beforehand.\n    \"\"\"\n    submission = pd.concat(preds_list, axis=0, ignore_index=True)\n\n    dup_mask = submission[\"Id\"].duplicated(keep=False)\n    if dup_mask.any():\n        dups = submission.loc[dup_mask].sort_values(\"Id\")\n        dup_ids = dups[\"Id\"].unique()\n\n        if verbose:\n            print(f\"[WARN] Found {len(dup_ids)} duplicated Id(s) after concat.\")\n            print(\"Sample duplicated rows:\")\n            print(dups.head(10))\n            \n        submission = submission.drop_duplicates(subset=\"Id\", keep=keep).reset_index(drop=True)\n\n        if verbose:\n            print(f\"[INFO] Deduplicated using keep='{keep}'. Final rows: {len(submission)}\")\n\n    assert not submission[\"Id\"].duplicated().any(), \"Deduplication failed: still duplicated Ids.\"\n\n    return submission","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Crafting The Baseline Model**\nI am using a median-based baseline in log1p space to handle the heavy-tailed and noisy nature of web traffic data. \n* Recent medians are for estimating the overall level\n* Weekday medians are for capturing weekly seasonality\n","metadata":{}},{"cell_type":"code","source":"@dataclass\nclass BaselineStats:\n    \"\"\"Sufficient statistics\"\"\"\n    pages: np.ndarray\n    last_date: pd.Timestamp\n    last28_median: np.ndarray\n    weekday_median: np.ndarray\n    global_median: float #acts as a fallback\n\ndef build_baseline_stats(\n    train_df: pd.DataFrame,\n    id_col: str = \"Page\",\n    lookback_weekday: int = 56, #num of days for estimating weekday seasonality \n    lookback_level: int = 28,) # to estimate an overall level\n    -> BaselineStats:\n\n    date_cols = detect_date_columns(train_df, id_col=id_col)\n    if len(date_cols) < max(lookback_weekday, lookback_level):\n        raise ValueError(\n            f\"Not enough date columns ({len(date_cols)}) for lookback windows.\"\n        )\n    \n    pages = train_df[id_col].astype(str).values #.astype(type) casts data from one type to another\n    dates = pd.to_datetime(date_cols) \n    last_date = dates.max()\n\n    y = train_df[date_cols].to_numpy(dtype=np.float32, copy=True)\n\n    y[y < 0] = np.nan\n\n    y_log = np.log1p(y)\n\n    # median of last 28 days\n    idx_level_start = len(date_cols) - lookback_level\n    last_level_window = y_log[:, idx_level_start:] \n    last28_median = np.nanmedian(last_level_window, axis=1) #for the last 28 days, what is the median in log1p space? \n\n    # median per weekday over last 56 days\n    idx_wk_start = len(date_cols) - lookback_weekday\n    recent_dates = dates[idx_wk_start:]\n    recent_y = y_log[:, idx_wk_start:] #this is an example of 2D slicing. so we take all rows but only the columns from the lookback weekday\n\n    weekday_median = np.full((recent_y.shape[0], 7), np.nan, dtype=np.float32) \n    for wd in range(7):\n        mask = (recent_dates.weekday == wd)\n        if mask.sum() == 0: #contigency\n            continue\n        weekday_median[:, wd] = np.nanmedian(recent_y[:, mask], axis=1)\n\n    # global median (log-space)\n    global_median = float(np.nanmedian(y_log)) #median log-traffic value across all pages and all days\n\n    del y, y_log, last_level_window, recent_y\n    gc.collect()\n\n    return BaselineStats(\n        pages=pages,\n        last_date=last_date,\n        last28_median=last28_median.astype(np.float32),\n        weekday_median=weekday_median.astype(np.float32),\n        global_median=global_median,\n    )\n\ndef predict_with_baseline(\n    stats: BaselineStats,\n    key_df: pd.DataFrame,\n    alpha_weekday: float = 0.7) -> pd.DataFrame:\n\n    page_to_idx = pd.Series(np.arange(len(stats.pages)), index=stats.pages)\n\n    key = key_df.copy()\n    key[\"idx\"] = page_to_idx.reindex(key[\"page\"]).to_numpy() #visualise this\n\n    wd = key[\"date\"].dt.weekday.to_numpy(dtype=np.int16)\n\n    pred_log = np.full(len(key), stats.global_median, dtype=np.float32)\n\n    valid = ~pd.isna(key[\"idx\"].to_numpy())\n    if valid.any():\n        idx = key.loc[valid, \"idx\"].astype(np.int64).to_numpy()\n        wd_valid = wd[valid]\n\n        wmed = stats.weekday_median[idx, wd_valid]\n        lmed = stats.last28_median[idx]\n\n        wmed_filled = np.where(np.isfinite(wmed), wmed, lmed)\n\n        lmed_filled = np.where(np.isfinite(lmed), lmed, stats.global_median)\n        wmed_filled = np.where(np.isfinite(wmed_filled), wmed_filled, lmed_filled)\n\n        pred_log_valid = alpha_weekday * wmed_filled + (1.0 - alpha_weekday) * lmed_filled\n        pred_log[valid] = pred_log_valid.astype(np.float32)\n\n    pred = np.expm1(pred_log).astype(np.float32)\n\n    pred = np.where(np.isfinite(pred), pred, 0.0)\n    pred = np.clip(pred, 0.0, None)\n\n    out = pd.DataFrame({\"Id\": key[\"id\"], \"Visits\": pred})\n    return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef visualise_single_page(train_df: pd.DataFrame, page: str, lookback_weekday=56, lookback_level=28):\n\n    date_cols = detect_date_columns(train_df)\n    dates = pd.to_datetime(date_cols)\n\n    y = train_df.loc[train_df[\"Page\"] == page, date_cols].to_numpy(dtype=np.float32).flatten()\n    y_log = np.log1p(y)\n\n    level_window = y_log[-lookback_level:]\n    weekday_window = y_log[-lookback_weekday:]\n    weekday_dates = dates[-lookback_weekday:]\n\n    last28 = np.nanmedian(level_window)\n    weekday_meds = {\n        wd: np.nanmedian(weekday_window[weekday_dates.weekday == wd])\n        for wd in range(7)\n    }\n\n    fig, axes = plt.subplots(3, 1, figsize=(12, 9), sharex=True)\n\n    axes[0].plot(dates, y)\n    axes[0].set_title(f\"Raw visits — {page}\")\n\n    axes[1].plot(dates, y_log)\n    axes[1].axhline(last28, color=\"red\", linestyle=\"--\", label=\"last-28 median (log1p)\")\n    axes[1].legend()\n    axes[1].set_title(\"log1p(visits)\")\n\n    axes[2].bar(range(7), [weekday_meds[w] for w in range(7)])\n    axes[2].set_xticks(range(7))\n    axes[2].set_xticklabels([\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\",\"Sat\",\"Sun\"])\n    axes[2].set_title(\"Weekday medians (log1p, last 56 days)\")\n\n    plt.tight_layout()\n    plt.show()\n\n\ndef visualise_prediction_blend(stats: BaselineStats, page: str, weekday: int, alpha=0.7):\n\n    idx = np.where(stats.pages == page)[0][0]\n\n    wmed = stats.weekday_median[idx, weekday]\n    lmed = stats.last28_median[idx]\n\n    pred_log = alpha * wmed + (1 - alpha) * lmed\n    pred = np.expm1(pred_log)\n\n    print(f\"Page: {page}\")\n    print(f\"Weekday: {weekday} (0=Mon … 6=Sun)\")\n    print(f\"weekday median (log1p): {wmed:.3f}\")\n    print(f\"last-28 median (log1p): {lmed:.3f}\")\n    print(f\"alpha_weekday: {alpha}\")\n    print(f\"→ blended log1p prediction: {pred_log:.3f}\")\n    print(f\"→ final Visits prediction: {pred:.1f}\")\n\ntrain_demo = pd.read_csv(TRAIN_1, compression=\"zip\")\ndate_cols = detect_date_columns(train_demo, id_col=\"Page\")\nlast_cols = date_cols[-120:]\ntrain_demo = train_demo[[\"Page\"] + last_cols]\n\npage_demo = train_demo[\"Page\"].iloc[0]\n\nvisualise_single_page(train_demo, page=page_demo)\n\nstats_demo = build_baseline_stats(train_demo, id_col=\"Page\", lookback_weekday=56, lookback_level=28)\nvisualise_prediction_blend(stats_demo, page=page_demo, weekday=0, alpha=0.7)\n\ndel train_demo, stats_demo\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **A quick validation**\nTo verify that the pipeline is behaving, I am evaluating the baseline on a time-based holdout on a random subset of pages. ","metadata":{}},{"cell_type":"code","source":"def quick_time_validation(\n    train_df: pd.DataFrame,\n    n_pages: int = 5000,\n    holdout_days: int = 60,\n    id_col: str = \"Page\") -> float:\n    \"\"\"Quick SMAPE on a page sample with a time holdout.\"\"\"\n\n    df = train_df.copy()\n    date_cols = detect_date_columns(df, id_col=id_col)\n\n    if len(df) > n_pages:\n        df = df.sample(n=n_pages, random_state=RANDOM_SEED)\n\n    train_cols = date_cols[:-holdout_days]\n    hold_cols  = date_cols[-holdout_days:]\n\n    truncated = pd.concat([df[[id_col]], df[train_cols]], axis=1)\n    stats = build_baseline_stats(truncated, id_col=id_col)\n\n    pages = df[id_col].astype(str).values\n    hold_dates = pd.to_datetime(hold_cols)\n\n    page_rep = np.repeat(pages, len(hold_dates))\n    date_rep = np.tile(hold_dates, len(pages))\n\n    key_like = pd.DataFrame({\n        \"id\": np.arange(len(page_rep), dtype=np.int64),\n        \"page\": page_rep,\n        \"date\": date_rep,\n    })\n\n    #we are still using what we trained the model with \n    pred_df = predict_with_baseline(stats, key_like)\n\n    # ground truth\n    y_true = df[hold_cols].to_numpy(dtype=np.float32)\n    y_true = y_true.reshape(-1)\n\n    y_pred = pred_df[\"Visits\"].to_numpy(dtype=np.float32)\n\n    y_true = np.where(np.isfinite(y_true), y_true, 0.0)\n\n    score = smape(y_true, y_pred)\n\n    del df, truncated, key_like, pred_df\n    gc.collect()\n\n    return score","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_val = pd.read_csv(TRAIN_1, compression=\"zip\")\nscore = quick_time_validation(train_val, n_pages=1000, holdout_days=60)\nprint(\"Quick SMAPE (n=1000 pages, holdout=60d):\", score)\n\ndel train_val\ngc.collect()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **The prediction**\n\nTraining data is processed sequentially, simultaneously would consume way too much memory. I merge predictions for each train-key pair later on. ","metadata":{}},{"cell_type":"code","source":"def process_chunk(train_path: str, key_path: str, id_col: str = \"Page\") -> pd.DataFrame:\n    \"\"\"Load a train chunk and its corresponding key, then produce predictions.\"\"\"\n\n    print(f\"Reading key: {os.path.basename(key_path)}\")\n    key = read_key(key_path)\n    print(\"Key shape:\", key.shape)\n\n    print(f\"Reading train: {os.path.basename(train_path)}\")\n    train = pd.read_csv(train_path)\n    print(\"Train shape:\", train.shape)\n\n    print(\"Building baseline statistics...\")\n    stats = build_baseline_stats(train, id_col=id_col)\n\n    print(\"Predicting key rows...\")\n    preds = predict_with_baseline(stats, key)\n\n    del train, key, stats\n    gc.collect()\n\n    return preds\n\npreds_1 = process_chunk(TRAIN_1, KEY_1)\npreds_2 = process_chunk(TRAIN_2, KEY_2)\n\nsubmission = concat_and_dedupe_predictions([preds_1, preds_2], keep=\"first\", verbose=True)\n    \nsubmission[\"Visits\"] = submission[\"Visits\"].fillna(0.0).astype(np.float32)\nsubmission.to_csv(OUT_PATH, index=False)\n\nprint(\"Saved:\", OUT_PATH)\nprint(submission.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}